{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from autoencoder_uriel import *\n",
    "from preprocess_uriel import *\n",
    "from visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\uriel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "import argparse\n",
    "\n",
    "#def model(): #:if __name__ == '__main__':\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    method = \"LDA_BERT\"\n",
    "    samp_size = len(documents)\n",
    "    ntopic = 10\n",
    "    \n",
    "\n",
    "    data = documents \n",
    "    data = data.fillna('')  # only the comments has NaN's\n",
    "    sentences, token_lists, idx_in = preprocess(documents['text'], samp_size=samp_size)\n",
    "    # Define the topic model object\n",
    "    tm = Topic_Model(k = ntopic, method = method)\n",
    "    # Fit the topic model by chosen method\n",
    "    tm.fit(sentences, token_lists)\n",
    "    # Evaluate using metrics\n",
    "    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
    "    print('Silhouette Score:', get_silhouette(tm))\n",
    "    #print('Topic Words:' , get_topic_words(token_lists, tm.cluster_model.labels_, k=ntopic))\n",
    "    outside_topics = get_topic_words(token_lists, tm.cluster_model.labels_, k=ntopic) \n",
    "    df_topics = pd.DataFrame({'Most important Words':outside_topics})\n",
    "    df_assigned_topic = pd.DataFrame({'Assigned topic':tm.cluster_model.labels_})\n",
    "    df_assigned_topic['index'] = range(len(df_assigned_topic))\n",
    "    documents_merged = documents.merge(df_assigned_topic, on='index', how='left')\n",
    "    print(df_topics)\n",
    "    print(documents_merged.columns)  \n",
    "    print(documents_merged.shape) \n",
    "                            ######show when need to print#######\n",
    "    #print(df_assigned_topic.value_counts())    ######show when need to print#######\n",
    "    sns.set(font_scale=1.4)\n",
    "    fig2 = plt.figure(\"Counting statements\", figsize=(8, 6), dpi=80)\n",
    "    documents_merged['Assigned topic'].value_counts().plot(kind = 'bar')###############add to csv\n",
    "    plt.xlabel(\"Topic number\", labelpad=14)\n",
    "    plt.ylabel(\"Count of statments\", labelpad=14)\n",
    "    plt.title(\"Count of Topics among statments\")\n",
    "    plt.show\n",
    "    \n",
    "    # visualize and save img\n",
    "    visualize(tm)\n",
    "    for i in range(tm.k):      # deleted line\n",
    "        get_wordcloud(tm, token_lists, i)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
